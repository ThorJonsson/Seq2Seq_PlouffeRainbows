\documentclass[a4paper,12pt]{article}

\usepackage{url}

\usepackage[T1]{fontenc}
\usepackage{color}				
\usepackage{soul}				
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{amssymb}



\begin{document}
\noindent

\subsection{Generative Networks - Learning Probability Distributions}


\subsection{Sequence to Sequence}


\section{Variational Autoencoders}

Wit
We consider learning some distribution \(P(X)\) by defining parametric families of densities \(p_{\theta}\)
merging combining the framework of neural networks with that of Bayesian inference. 

Bayesian inference solves the problem of approximating some probability distribution \(P(X)\) by considering the marginal likelihood given some latent variable \(z\) such that, 

\begin{equation}
P(X) = \dfrac{P(X \vert Z)}{\int_z P(X \vert z) dz}
\end{equation}

A variational autoencoder consists of an autoencoder where,
\begin{itemize}
\item The encoder encodes the parameters of the unknown prior distribution into a latent variable via reparameterization of a standard Gaussian.
\item The decoder decodes the latent variable into a reconstruction of the datapoint. 
\end{itemize}

The problem that the authors consider is that of performing Bayesian variational inference and unsupervised learning, in an efficient manner, using directed probabilistic models whose posterior distribution is generally intractable.
For this, the authors consider the situation of an i.i.d. set of data points \(x^{(i)}\) which have been generated by some unknown, parametrised families of distributions given by \(p_{\theta^*}(z^{(i)})\) and \(p_{\theta^*}(x^{(i)} \vert z^{(i)})\).
It is assumed that these distributions involves some latent variable \(z^{(i)}\). These variables and the true families of distributions are unknown to us, but using the data points \(x^{(i)}\) we would like to obtain an estimate of the generating process which produces them. This can thus allow us to mimic the unknown random process which the aforementioned families of distributions describe.

%learning the parameters of a distribution describing the latent variables of a generative process. 
The problem of intractability arises when we consider modeling the prior distribution over the latent variables. 
By Bayes theorem the true posterior density which the decoder aims to model can be written as
\begin{equation}
p_{\theta}(z \vert x) = \dfrac{p_\theta(x \vert z)p_\theta(z)}{p_\theta(x)}
\end{equation}
where the marginal likelihood is given by
\[p_{\theta}(x) = \int p_{\theta}(z)p_{\theta}(x\vert z) dz.\]
This integral is in general analytically intractable for complex probability distributions.
Previously known methods used for variational inference often involve an estimation of the intractable integral by the usage of Monte Carlo methods. For very large data sets such methods become impractical as they are computationally expensive. 

\subsection{Examples of Usage}
Recently a number of articles have shown the application of this technique in Deep Learning, for generative models for both computer vision and natural language processing.

Some of the future directions that have been taken are to train other deep generative models using the learning algorithm proposed in this paper. Recent notable examples include the DRAW paper by Gregor et al.

Amongst the most interesting follow ups to this work, in the field of generative models, is the Deep Recurrent Attention Writer proposed by Gregor et al. By using a recurrent encoder-decoder model which incorporates attention, they are able to select different subsections/patches in the image for an iterative reading and writing process.

More recently, Rezende et al consider an alternative attention mechanism to that of Gregor et al . Theirs consists of convolving an input image with learned kernels rather than iteratively selecting patches. The authors claim that this allows the model to be spatially invariant to image features when determining which area of the image should have attention. The resultant model produces stunning images which are particularly  interesting for their lack of blurriness. Blurriness has been thought to be one downsides of image generation with variational autoencoders as opposed to using generative adversarial networks.

Recently, Miao et al proposed a variational framework specifically for doing text processing. The authors propose two different neural variational frameworks in the context of neural language modelling.  Firstly, how documents can be modelled by learning the variational parameters from a bag of words, considering those parameters as topics that allow for document reconstruction. Secondly, they propose a variational framework of conversation modeling.


\subsection{Generative Adversarial Networks}

Generative Adversarial Networks (GANs) consist of two components, a generative network which generates data from random noise and a discriminative network which discriminates between true data and data generated by the generative network. 






\section{The Plouffe Graph}
\subsection{Group Theory Background}

A group, \(G\) is a set with an operation, \(\ast\), such that the following properties hold, 
\begin{itemize}
\item The set is closed with respect to the operation. That is if \(a,b \in G\) then \(a \ast b \in G\)
\item There exists an identity element \(e\) such that for all \(a  \in G\) we have \(a \ast e = a\)
\item For all \(a \in G\) there exists an element \( a^{-1} \) such that \(a \ast a^{-1} = e\). 
\(a^{-1}\) is called the inverse of \(a\).
\item The operation for the group is associative, meaning that \( (a \ast b) \ast c = a \ast ( b \ast c) \)
\end{itemize}

Example:

\begin{itemize} 
\item The set of all integers with addition modulo \(N\) for \(N \in \mathbb{N}\) is a group of \(N\) elements.

\item The set of all complex roots of the polynomial \(X^{N} + 1\) with multiplication is a group of \(N\) elements. 
\end{itemize}

It is easy to show that the two examples given above are the same group!

\subsection{Plouffe Graph Definition}

Let \(N, k \in \mathbb{N}\) and let \(\zeta = \exp\left(\dfrac{2 \pi i}{N}\right)\).
The set of all exponents of \(\zeta\) is the set \(C_N = \{ \zeta^j : j \in \mathbb{N}\}\).

\(C_N\) is also the set of all roots of the polynomial \( \zeta^N  + 1 = 0 \). Furthermore it's the multiplicative group isomorphic to the group of residue classes modulo \(N\).

The Plouffe Graph for a given \(k, N\) is given by,

\[ \mathcal{P}^{(k)}_N = \left( C_N, \{ \left( \zeta^j, \zeta^{jk} \right) : j \in \mathbb{Z}_N \} \right) \]

We can consider sequences of such graphs by iterating either \(k\) or \(N\). 

We can also represent each the graphs in a condensed manner with a vector representation by, 

\[ \mathcal{P}^{(k)}_N = \left(\zeta^{jk}_j\right)^{N}_{j=1} \]

The sequence of such graphs for incremental values of \(k\) is very beautiful. Furthermore one can show that with the Hadamard product as an operation the set \[\{ \mathcal{P}^{(k)}_N : k \in \mathbb{R} \}\] forms a cyclic group.

\section{The Mandelbrot Set}

The Mandelbrot is the set of all \(c \in \mathbb{C}\) such that the iteration sequence given by,

\[
z_{i+1} = z^2_i + c , \hspace{2cm} z_0 = 0 
\]

does not diverge.
\section{2D Cellular Automata}


\end{document}