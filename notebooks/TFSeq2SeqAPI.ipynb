{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Working with TF commit 24466c2e6d32621cd85f0a78d47df6eed2c5c5a6\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.layers import safe_embedding_lookup_sparse as embedding_lookup_unique\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n",
    "\n",
    "import helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-0e532c7d4a60>, line 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-0e532c7d4a60>\"\u001b[0;36m, line \u001b[0;32m70\u001b[0m\n\u001b[0;31m    def _set_decoder():\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Seq2SeqNetwork():\n",
    "    ''' \n",
    "     Basic unidirectional seq2seq model with no attention\n",
    "     For now we assume all sequences are of the same length\n",
    "     This is a simplification offered to us by the Fibonacci task\n",
    "    '''\n",
    "    PAD = 0\n",
    "    EOS = 1\n",
    "    EOS_SLICE = tf.ones([1, self.batch_size], dtype=tf.int32) * EOS\n",
    "    PAD_SLICE = tf.ones([1, self.batch_size], dtype=tf.int32) * PAD\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_config, data_config):\n",
    "        \n",
    "        self.encoder_cell = model_config.encoder_cell\n",
    "        self.decoder_cell = model_config.decoder_cell\n",
    "        \n",
    "        self.sequence_length = data_config.sequence_length\n",
    "        self.embedding_size = model_config.embedding_size\n",
    "        self.alphabet_size = data_config.alphabet_size\n",
    "        \n",
    "        self._make_graph()\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def decoder_hidden_units(self):\n",
    "        return self.decoder_cell.output_size\n",
    "    \n",
    "    \n",
    "    def _set_target(self, target):\n",
    "        \"\"\"\n",
    "        During training, `decoder_targets`\n",
    "        and decoder logits. This means that their shapes should be compatible.\n",
    "\n",
    "        Here we do a bit of plumbing to set this up.\n",
    "        \"\"\"\n",
    "        target = tf.placeholder()\n",
    "        \n",
    "        with tf.name_scope('DecoderTrainFeeds'):\n",
    "            \n",
    "            # Put the end of sentence token on top of all the decoder_targets\n",
    "            # For the copying task these are the reverted inputs following EOS\n",
    "            decoder_input = tf.concat([EOS_SLICE, target], axis=0)\n",
    "\n",
    "            decoder_target = tf.concat([target, PAD_SLICE], axis=0)\n",
    "            \n",
    "            eos_mask = tf.one_hot(self.sequence_length,\n",
    "                                  self.sequence_length + 1,\n",
    "                                  on_value = EOS, \n",
    "                                  off_value = PAD,\n",
    "                                  dtype = tf.int32)\n",
    "            \n",
    "            eos_mask = tf.transpose(eos_mask, [1, 0])\n",
    "\n",
    "            # hacky way using one_hot to put EOS symbol at the end of target sequence\n",
    "            decoder_target = tf.add(decoder_target, eos_mask)\n",
    "\n",
    "            loss_weights = tf.ones(shape=(self.batch_size, self.sequence_length), \n",
    "                                   dtype=tf.float32, \n",
    "                                   name=\"loss_weights\")\n",
    "            \n",
    "            self.decoder_input = decoder_input\n",
    "            self.decoder_train_target = decoder_train_target\n",
    "            self.loss_weights = loss_weights\n",
    "             \n",
    "            \n",
    "    def _set_decoder():\n",
    "        \n",
    "        with tf.variable_scope(\"Decoder\") as scope:\n",
    "            \n",
    "            def output_fn(outputs):\n",
    "                return tf.contrib.layers.linear(outputs, self.alphabet_size, scope=scope)\n",
    "            \n",
    "            \n",
    "            decode = seq2seq.simple_decoder_fn_train(encoder_state = self.encoder_state)\n",
    "            sample = seq2seq.simple_decoder_fn_inference(output_fn=output_fn,\n",
    "                                                        encoder_state = self.encoder_state,\n",
    "                                                        embeddings = self.embedding_matrix,\n",
    "                                                        start_of_sequence_id = self.EOS\n",
    "                                                        end_of_sequence_id = self.EOS, \n",
    "                                                        maximum_length = tf.reduce_max(self.encoder_inputs) + 3,\n",
    "                                                        num_decoder_symbols = self.alphabet_size)\n",
    "        \n",
    "            decoder_train = (\n",
    "        \n",
    "                 seq2seq.dynamic_rnn_decoder(cell = self.decoder_cell,\n",
    "                                        decoder_fn = decode,\n",
    "                                        inputs = self.decoder_embedding\n",
    "                                        sequence_length = self.sequence_length,\n",
    "                                        time_major = True,\n",
    "                                        scope = scope)\n",
    "            )\n",
    "        \n",
    "            self.decoder_output_train = decoder_train[0]\n",
    "            self.decoder_state_train = decoder_train[1]\n",
    "            self.decoder_context_state_train = decoder_train[2]\n",
    "            \n",
    "            self.decoder_logits_train = output_fn(self.decoder_output)\n",
    "            self.decoder_prediction_train = tf.argmax(self.decoder_logits,\n",
    "                                                axis = -1,\n",
    "                                                name = 'decoder_prediction')\n",
    "            # explain\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "            decoder_infer = (\n",
    "                \n",
    "                seq2seq.dynamic_rnn_decoder(cell = self.decoder_cell,\n",
    "                                            decoder_fn = sample,\n",
    "                                            time_major = True,\n",
    "                                            scope = scope)\n",
    "                \n",
    "            )\n",
    "            \n",
    "            self.decoder_logits_inference = decoder_infer[0]\n",
    "            self.decoder_state_inference = decoder_infer[1]\n",
    "            self.decoder_context_state_inference = decoder_infer[2]\n",
    "            \n",
    "            self.decoder_prediction_inference = tf.argmax(self.decoder_logits_inference,\n",
    "                                                          axis = -1,\n",
    "                                                          name = 'decoder_prediction_inference')\n",
    "            \n",
    "    \n",
    "    def _init_optimizer(self):\n",
    "        logits = tf.transpose(self.decoder_logits_train, [1, 0, 2])\n",
    "        targets = tf.transpose(self.decoder_train_target, [1, 0])\n",
    "        self.loss = seq2seq.sequence_loss(logits = logits,\n",
    "                                          targets = targets,\n",
    "                                          weights = self.loss_weights)\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "        \n",
    "    def make_train_inputs(self, input_seq, target_seq):\n",
    "        inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "        targets_, targets_length_ = helpers.batch(target_seq)\n",
    "        return {\n",
    "            self.encoder_inputs: inputs_,\n",
    "            self.encoder_inputs_length: inputs_length_,\n",
    "            self.decoder_targets: targets_,\n",
    "            self.decoder_targets_length: targets_length_,\n",
    "        }\n",
    "\n",
    "    def make_inference_inputs(self, input_seq):\n",
    "        inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "        return {\n",
    "            self.encoder_inputs: inputs_,\n",
    "            self.encoder_inputs_length: inputs_length_,\n",
    "        }\n",
    "\n",
    "\n",
    "    def _set_encoder():\n",
    "        \n",
    "        with tf.variable_scope('Encoder') as scope:\n",
    "            (self.encoder_output, self.encoder_state) = (\n",
    "                    \n",
    "                tf.nn.dynamic_rnn(cell = self.encoder_cell,\n",
    "                                  inputs = self.encoder_embedding,\n",
    "                                  sequence_length = self.sequence_length,\n",
    "                                  time_major = True,\n",
    "                                  dtype = tf.float32)\n",
    "            )\n",
    "            \n",
    "    def _set_embeddings(self):\n",
    "        with tf.variable_scope(\"embedding\") as scope:\n",
    "\n",
    "            # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "            sqrt3 = math.sqrt(3)\n",
    "            initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "            self.embedding_matrix = tf.get_variable(name=\"embedding_matrix\",\n",
    "                                        shape=(self.alphabet_size, self.embedding_size),\n",
    "                                        initializer=initializer,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "            self.encoder_embedded = tf.nn.embedding_lookup(self.embedding_matrix, \n",
    "                                                           self.encoder_inputs)\n",
    "\n",
    "            self.decoder_embedded = tf.nn.embedding_lookup(self.embedding_matrix,\n",
    "                                                           self.decoder_inputs)\n",
    "\n",
    "            \n",
    "    def _make_graph(self):\n",
    "        \n",
    "        # init placeholders\n",
    "        self.input = tf.placeholder(shape = (self.sequence_length, self.batch_size),\n",
    "                                            dtype = tf.int32,\n",
    "                                            name  = 'encoder_inputs')\n",
    "        \n",
    "        self.target = tf.placeholder(shape = (self.sequence_length, self.batch_size),\n",
    "                                             dtype = tf.int32,\n",
    "                                             name = 'decoder_target')\n",
    "        \n",
    "        self._set_embeddings()\n",
    "        \n",
    "        self._set_encoder()\n",
    "        \n",
    "        self._set_decoder()\n",
    "        \n",
    "        self._set_optimizer()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_seq2seq_model(**kwargs):\n",
    "    args = dict(encoder_cell=LSTMCell(10),\n",
    "                decoder_cell=LSTMCell(20),\n",
    "                vocab_size=10,\n",
    "                embedding_size=10,\n",
    "                attention=True,\n",
    "                bidirectional=True,\n",
    "                debug=False)\n",
    "    args.update(kwargs)\n",
    "    return Seq2SeqModel(**args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_on_copy_task(session, model,\n",
    "                       length_from=3, length_to=8,\n",
    "                       vocab_lower=2, vocab_upper=10,\n",
    "                       batch_size=100,\n",
    "                       max_batches=5000,\n",
    "                       batches_in_epoch=1000,\n",
    "                       verbose=True):\n",
    "\n",
    "    batches = helpers.random_sequences(length_from=length_from, length_to=length_to,\n",
    "                                       vocab_lower=vocab_lower, vocab_upper=vocab_upper,\n",
    "                                       batch_size=batch_size)\n",
    "    loss_track = []\n",
    "    try:\n",
    "        for batch in range(max_batches+1):\n",
    "            batch_data = next(batches)\n",
    "            fd = model.make_train_inputs(batch_data, batch_data)\n",
    "            _, l = session.run([model.train_op, model.loss], fd)\n",
    "            loss_track.append(l)\n",
    "\n",
    "            if verbose:\n",
    "                if batch == 0 or batch % batches_in_epoch == 0:\n",
    "                    print('batch {}'.format(batch))\n",
    "                    print('  minibatch loss: {}'.format(session.run(model.loss, fd)))\n",
    "                    for i, (e_in, dt_pred) in enumerate(zip(\n",
    "                            fd[model.encoder_inputs].T,\n",
    "                            session.run(model.decoder_prediction_train, fd).T\n",
    "                        )):\n",
    "                        print('  sample {}:'.format(i + 1))\n",
    "                        print('    enc input           > {}'.format(e_in))\n",
    "                        print('    dec train predicted > {}'.format(dt_pred))\n",
    "                        if i >= 2:\n",
    "                            break\n",
    "                    print()\n",
    "    except KeyboardInterrupt:\n",
    "        print('training interrupted')\n",
    "\n",
    "    return loss_track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    import sys\n",
    "\n",
    "    if 'fw-debug' in sys.argv:\n",
    "        tf.reset_default_graph()\n",
    "        with tf.Session() as session:\n",
    "            model = make_seq2seq_model(debug=True)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(model.decoder_prediction_train)\n",
    "            session.run(model.decoder_prediction_train)\n",
    "\n",
    "    elif 'fw-inf' in sys.argv:\n",
    "        tf.reset_default_graph()\n",
    "        with tf.Session() as session:\n",
    "            model = make_seq2seq_model()\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            fd = model.make_inference_inputs([[5, 4, 6, 7], [6, 6]])\n",
    "            inf_out = session.run(model.decoder_prediction_inference, fd)\n",
    "            print(inf_out)\n",
    "\n",
    "    elif 'train' in sys.argv:\n",
    "        tracks = {}\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            model = make_seq2seq_model(attention=True)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            loss_track_attention = train_on_copy_task(session, model)\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            model = make_seq2seq_model(attention=False)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            loss_track_no_attention = train_on_copy_task(session, model)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(loss_track)\n",
    "        print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))\n",
    "\n",
    "    else:\n",
    "        tf.reset_default_graph()\n",
    "        session = tf.InteractiveSession()\n",
    "        model = make_seq2seq_model(debug=False)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        fd = model.make_inference_inputs([[5, 4, 6, 7], [6, 6]])\n",
    "\n",
    "        inf_out = session.run(model.decoder_prediction_inference, fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, GRUCell\n",
    "from model_new import Seq2SeqModel, train_on_copy_task\n",
    "import pandas as pd\n",
    "import helpers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # with bidirectional encoder, decoder state size should be\n",
    "    # 2x encoder state size\n",
    "    model = Seq2SeqModel(encoder_cell=LSTMCell(10),\n",
    "                         decoder_cell=LSTMCell(20), \n",
    "                         vocab_size=10,\n",
    "                         embedding_size=10,\n",
    "                         attention=True,\n",
    "                         bidirectional=True,\n",
    "                         debug=False)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    train_on_copy_task(session, model,\n",
    "                       length_from=3, length_to=8,\n",
    "                       vocab_lower=2, vocab_upper=10,\n",
    "                       batch_size=100,\n",
    "                       max_batches=3000,\n",
    "                       batches_in_epoch=1000,\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_tracks = dict()\n",
    "\n",
    "def do_train(session, model):\n",
    "    return train_on_copy_task(session, model,\n",
    "                              length_from=3, length_to=8,\n",
    "                              vocab_lower=2, vocab_upper=10,\n",
    "                              batch_size=100,\n",
    "                              max_batches=5000,\n",
    "                              batches_in_epoch=1000,\n",
    "                              verbose=False)\n",
    "\n",
    "def make_model(**kwa):\n",
    "    args = dict(cell_class=LSTMCell,\n",
    "                num_units_encoder=10,\n",
    "                vocab_size=10,\n",
    "                embedding_size=10,\n",
    "                attention=False,\n",
    "                bidirectional=False,\n",
    "                debug=False)\n",
    "    args.update(kwa)\n",
    "    \n",
    "    cell_class = args.pop('cell_class')\n",
    "    \n",
    "    num_units_encoder = args.pop('num_units_encoder')\n",
    "    num_units_decoder = num_units_encoder\n",
    "    \n",
    "    if args['bidirectional']:\n",
    "        num_units_decoder *= 2\n",
    "    \n",
    "    args['encoder_cell'] = cell_class(num_units_encoder)\n",
    "    args['decoder_cell'] = cell_class(num_units_decoder)\n",
    "    \n",
    "    return Seq2SeqModel(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=True, cell_class=LSTMCell)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    t0 = time.time()\n",
    "    lstm_track = do_train(session, model)\n",
    "    lstm_took = time.time() - t0\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=True, cell_class=GRUCell)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    t0 = time.time()\n",
    "    gru_track = do_train(session, model)\n",
    "    gru_took = time.time() - t0\n",
    "    \n",
    "gru = pd.Series(gru_track, name='gru')\n",
    "lstm = pd.Series(lstm_track, name='lstm')\n",
    "tracks_batch = pd.DataFrame(dict(lstm=lstm, gru=gru))\n",
    "tracks_batch.index.name = 'batch'\n",
    "\n",
    "gru.index = gru.index / gru_took\n",
    "lstm.index = lstm.index / lstm_took\n",
    "tracks_time = pd.DataFrame(dict(lstm=lstm, gru=gru)).ffill()\n",
    "tracks_time.index.name = 'time (seconds)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=False, attention=False)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['forward encoder, no attention'] = do_train(session, model)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=False)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['bidirectional encoder, no attention'] = do_train(session, model)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=False, attention=True)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['forward encoder, with attention'] = do_train(session, model)\n",
    "\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "with tf.Session() as session:\n",
    "    model = make_model(bidirectional=True, attention=True)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_tracks['bidirectional encoder, with attention'] = do_train(session, model)\n",
    "\n",
    "pd.DataFrame(loss_tracks).plot(figsize=(13, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py2.7tf1.0]",
   "language": "python",
   "name": "Python [py2.7tf1.0]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
